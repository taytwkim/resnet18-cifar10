# ResNet 18 

Toy workload to experiment with neural networks.

## üöÄ Setup

1. Activate a venv.
```bash!
python3 -m venv .venv
source .venv/bin/activate
python3 -m pip install --upgrade pip
```

2. Install dependencies.

* If on CPU or MPS (Apple Silicon):
```bash!
pip install -r requirements.txt
```

* If on NVIDIA, prefer CUDA wheels:
```bash!
pip install torch torchvision numpy pillow --index-url https://download.pytorch.org/whl/cu121
```

3. Train model (single node).

* For CPU or MPS:
```bash!
python3 train.py --epochs 5 --batch-size 128
```

* On NVIDIA GPUs, enable mixed precision (amp) for speed:
```
python3 train.py --epochs 20 --batch-size 256 --amp --workers 4
```

## üöÇ Arguments
```bash!
python3 train.py [--epochs <INT>] [--batch-size <INT>] [--lr <FLOAT>] 
                 [--data <PATH>] [--out-dir <DIR>]
                 [--workers <INT>] [--amp] [--label-smoothing <FLOAT>]
                 [--warmup <INT>] [--seed <INT>] [--resume <PATH>]
                 [--save-every <INT>]
```

  * `--epochs`: Number of full passes over the training set.
  * `--batch-size`: Per-GPU/per-process mini-batch size. Global = batch_size √ó world_size.
  * `--lr`: Base learning rate per GPU.
  * `--data`: CIFAR-10 data directory; rank 0 downloads here if missing.
  * `--out-dir`: Output directory for checkpoints/final weights (rank 0 only).
  * `--workers`: DataLoader workers per process. CPU: 0‚Äì2; GPU: 4‚Äì8 typical.
  * `--amp`: Enable mixed precision on CUDA (ignored on CPU/MPS).
  * `--label-smoothing`: Label smoothing for cross-entropy (e.g., 0.1).
  * `--warmup`: Optimizer steps (not epochs) of linear warmup before cosine decay.
  * `--seed`: Base RNG seed (offset by rank).
  * `--resume`: Resume from checkpoint produced by this script.
  * `--save-every`: Also save a snapshot every N epochs (0 disables).

## üìÅ Directory
```
resnet18-cifar10/
‚îú‚îÄ train.py
‚îú‚îÄ data/
‚îú‚îÄ artifacts/
‚îî‚îÄ requirements.txt
```

* `data/` [Not tracked by git]
    * `cifar-10-python.tar.gz`: the original compressed dataset that `torchvision` downloads.
    * `cifar-10-batches-py/`: extracted from the tarball. This is what `torchvision.datasets.CIFAR10` actually reads.
        * `data_batch_1` ‚Ä¶ `data_batch_5`: 5 training batches - 10,000 images each.
        * `test_batch`: 10,000 test samples.

* `artifacts/` [Empty directory tracked by git]
    * `*_final_weights.pt` (weights only)
        * Use for inference or fine-tuning from scratch LR.
        * Contains just `model.state_dict()`.
    * `*_best.pt` (full checkpoint)
        * Use to resume training with identical optimizer dynamics.
        * Contains model weights, optimizer state (e.g., momentum), scheduler state (e.g., where you are on the LR curve), plus metadata like epoch and best accuracy.
